{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "insolvencyscraper_gazetteSEQ.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0l9Tatdoa1YI",
        "colab_type": "text"
      },
      "source": [
        "# Scraping the Gazette for insolvency notices (sequential)\n",
        "\n",
        "Search results are at https://m.thegazette.co.uk/insolvency?categorycode=G205010000&results-page=1 for company insolvencies. Personal insolvencies can be searched separately.\n",
        "\n",
        "Each notice has a number like so: https://m.thegazette.co.uk/notice/3552846\n",
        "\n",
        "Simply cycling through catches all types of notice including both business and personal."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QdwQFP4wzgFu",
        "colab_type": "text"
      },
      "source": [
        "## Install the libraries\n",
        "\n",
        "Firstly we install scraperwiki and some other libraries we might need."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ryl-SH8zcRr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Install libraries\n",
        "!pip install scraperwiki\n",
        "import scraperwiki\n",
        "import lxml.html\n",
        "!pip install cssselect\n",
        "import cssselect"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v4pRLNZ4Ejwu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#import pandas\n",
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4-s3AvZ3zlFV",
        "colab_type": "text"
      },
      "source": [
        "## Scrape a single page\n",
        "\n",
        "Next we see if we can scrape a single page."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1C_dyVPPEBhR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Store a URL we want to work back from\n",
        "latesturl = \"https://m.thegazette.co.uk/notice/3391189\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u15xiIhqzy2_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "8e5c2c8f-ae54-4e73-a987-2f532c0c563d"
      },
      "source": [
        "print(\"scraping\",latesturl)\n",
        "#use the scrape function on that url\n",
        "html = scraperwiki.scrape(latesturl)\n",
        "# turn that variable's contents into an lxml object, making it easier to drill into\n",
        "root = lxml.html.fromstring(html) \n",
        "\n",
        "#The basic info is all in <div class=\"notice-data\"> and then <dd>\n",
        "noticedata = root.cssselect('div.notice-data dd')\n",
        "#How many matches\n",
        "noticecategory = noticedata[0].text_content()\n",
        "noticetype = noticedata[1].text_content()\n",
        "pubdate = noticedata[2].text_content()\n",
        "print(noticecategory, noticetype)\n",
        "#The notice codes provide further detail\n",
        "#See https://www.thegazette.co.uk/noticecodes\n",
        "#We need to identify which ≤dt> has 'Notice code' to scrape the relevant <dd>\n",
        "noticedlabels = root.cssselect('div.notice-data dt')\n",
        "#Measure the list so we can loop\n",
        "howlong = len(noticedlabels)\n",
        "#Loop through numbers\n",
        "#Set a default value first\n",
        "noticecode = \"NO DATA\"\n",
        "for i in range(0,howlong):\n",
        "  #Check the text at that index in that list of labels\n",
        "  if noticedlabels[i].text_content() == 'Notice code:':\n",
        "    #If it does, grab the corresponding index from the other list of values\n",
        "    noticecode = noticedata[i].text_content()\n",
        "\n",
        "#Create a dictionary\n",
        "record = {}\n",
        "record['latesturl'] = latesturl\n",
        "record['noticecategory'] = noticecategory\n",
        "record['noticetype'] = noticetype\n",
        "record['noticecode'] = noticecode\n",
        "\n",
        "#Other data is in helpful data-gazettes attributes like so:\n",
        "companynames = root.cssselect('[data-gazettes=\"CompanyName\"]')\n",
        "if len(companynames)>0:\n",
        "  companyname = companynames[0].text_content()\n",
        "else:\n",
        "  companyname = \"NO DATA\"\n",
        "\n",
        "compnums = root.cssselect('[data-gazettes=\"CompanyNumber\"]')\n",
        "if len(compnums)>0:\n",
        "  compnum = compnums[0].text_content()\n",
        "else:\n",
        "  compnum = \"NO DATA\"\n",
        "\n",
        "record['compnum'] = compnum\n",
        "record['companyname'] = companyname\n",
        "\n",
        "addresses = root.cssselect('[data-gazettes=\"CompanyRegisteredOffice\"]')\n",
        "if len(addresses)>0:\n",
        "  address = addresses[0].text_content()\n",
        "else:\n",
        "  address = \"NO DATA\"\n",
        "\n",
        "typesofliq = root.cssselect('[data-gazettes=\"TypeOfLiquidation\"]')\n",
        "if len(typesofliq)>0:\n",
        "  typeofliq = typesofliq[0].text_content()\n",
        "else:\n",
        "  typeofliq = \"NO DATA\"\n",
        "\n",
        "\n",
        "record['compnum'] = compnum\n",
        "record['address'] = address\n",
        "record['typeofliq'] = typeofliq\n",
        "\n",
        "print(record)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "scraping https://m.thegazette.co.uk/notice/3391189\n",
            "Corporate Insolvency Appointment of Liquidators\n",
            "{'latesturl': 'https://m.thegazette.co.uk/notice/3391189', 'noticecategory': 'Corporate Insolvency', 'noticetype': 'Appointment of Liquidators', 'noticecode': '2432', 'compnum': '07534757', 'companyname': 'KERR ADVISORY LIMITED', 'address': 'Registered office: Westminster Business Centre, 10 Great North Way, Nether Poppleton, York, YO26 6RB', 'typeofliq': \"Type of Liquidation: Members' Voluntary Liquidation\"}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KqRl_e_P6X82",
        "colab_type": "text"
      },
      "source": [
        "## Create a function\n",
        "\n",
        "That works, so let's store it in a function. We also add some lines for personal insolvency."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ovnVTMn56amE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def scrapedetail(url):\n",
        "  print(\"scraping\",url)\n",
        "  #use the scrape function on that url\n",
        "  html = scraperwiki.scrape(url)\n",
        "  # turn that variable's contents into an lxml object, making it easier to drill into\n",
        "  root = lxml.html.fromstring(html) \n",
        "  #The basic info is all in <div class=\"notice-data\"> and then <dd>\n",
        "  noticedata = root.cssselect('div.notice-data dd')\n",
        "  #The first and second and third items contain data we store\n",
        "  noticecategory = noticedata[0].text_content()\n",
        "  noticetype = noticedata[1].text_content()\n",
        "  pubdate = noticedata[2].text_content()\n",
        "  #The notice codes provide further detail\n",
        "  #See https://www.thegazette.co.uk/noticecodes\n",
        "  #We need to identify which ≤dt> has 'Notice code' to scrape the relevant <dd>\n",
        "  noticedlabels = root.cssselect('div.notice-data dt')\n",
        "  #Measure the list so we can loop\n",
        "  howlong = len(noticedlabels)\n",
        "  #Loop through numbers\n",
        "  #Set a default value first\n",
        "  noticecode = \"NO DATA\"\n",
        "  for i in range(0,howlong):\n",
        "    #Check the text at that index in that list of labels\n",
        "    if noticedlabels[i].text_content() == 'Notice code:':\n",
        "      #If it does, grab the corresponding index from the other list of values\n",
        "      noticecode = noticedata[i].text_content()\n",
        "  #Create a dictionary\n",
        "  record = {}\n",
        "  record['url'] = url\n",
        "  record['noticecategory'] = noticecategory\n",
        "  record['noticetype'] = noticetype\n",
        "  record['noticecode'] = noticecode\n",
        "  record['pubdate'] = pubdate\n",
        "  #Other data is in helpful data-gazettes attributes like so:\n",
        "  companynames = root.cssselect('[data-gazettes=\"CompanyName\"]')\n",
        "  if len(companynames)>0:\n",
        "    companyname = companynames[0].text_content()\n",
        "  else:\n",
        "    companyname = \"NO DATA\"\n",
        "  compnums = root.cssselect('[data-gazettes=\"CompanyNumber\"]')\n",
        "  if len(compnums)>0:\n",
        "    compnum = compnums[0].text_content()\n",
        "  else:\n",
        "    compnum = \"NO DATA\"\n",
        "  record['compnum'] = compnum\n",
        "  record['companyname'] = companyname\n",
        "  addresses = root.cssselect('[data-gazettes=\"CompanyRegisteredOffice\"]')\n",
        "  if len(addresses)>0:\n",
        "    address = addresses[0].text_content()\n",
        "  else:\n",
        "    address = \"NO DATA\"\n",
        "  dobs = root.cssselect('[data-gazettes=\"BirthDetails\"]')\n",
        "  if len(dobs)>0:\n",
        "    dob = dobs[0].text_content()\n",
        "  else:\n",
        "    dob = \"NO DATA\"\n",
        "  persdetails = root.cssselect('[data-gazettes=\"PersonDetails\"]')\n",
        "  if len(persdetails)>0:\n",
        "    persdetail = persdetails[0].text_content()\n",
        "  else:\n",
        "    persdetail = \"NO DATA\"\n",
        "  record['compnum'] = compnum\n",
        "  record['address'] = address\n",
        "  record['dob'] = dob\n",
        "  record['persdetail'] = persdetail\n",
        "\n",
        "  typesofliq = root.cssselect('[data-gazettes=\"TypeOfLiquidation\"]')\n",
        "  if len(typesofliq)>0:\n",
        "    typeofliq = typesofliq[0].text_content()\n",
        "  else:\n",
        "    typeofliq = \"NO DATA\"\n",
        "  record['typeofliq'] = typeofliq\n",
        "  return(record)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FxkQXRWY66E6",
        "colab_type": "text"
      },
      "source": [
        "Now to run that."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MIXvVg7d67EX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7c46bdfe-dd12-4913-9d55-a3ca7aac8565"
      },
      "source": [
        "testrecord = scrapedetail(\"https://m.thegazette.co.uk/notice/3391189\")\n",
        "print(testrecord)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "scraping https://m.thegazette.co.uk/notice/3391189\n",
            "{'url': 'https://m.thegazette.co.uk/notice/3391189', 'noticecategory': 'Corporate Insolvency', 'noticetype': 'Appointment of Liquidators', 'noticecode': '2432', 'pubdate': '23 September 2019', 'compnum': '07534757', 'companyname': 'KERR ADVISORY LIMITED', 'address': 'Registered office: Westminster Business Centre, 10 Great North Way, Nether Poppleton, York, YO26 6RB', 'dob': 'NO DATA', 'persdetail': 'NO DATA', 'typeofliq': \"Type of Liquidation: Members' Voluntary Liquidation\"}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_-wnaHvc7Ker",
        "colab_type": "text"
      },
      "source": [
        "So that works. Now to create a loop and prepare to save the results."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9T8-XjTc7PeH",
        "colab_type": "text"
      },
      "source": [
        "## Create a loop\n",
        "\n",
        "The last number is 3391187 so we need to start from there and go backwards. We will try going 10 back from there to begin with.\n",
        "\n",
        "The latest is 3581867 so we'll go back from there too."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-HIQGH3C7Zdt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "startnum = 3581867\n",
        "endnum = startnum-10\n",
        "#When going backwards you have to add a negative 'step' argument\n",
        "for i in range(startnum, endnum, -1):\n",
        "  #We have to convert to string to add to URL\n",
        "  print('https://m.thegazette.co.uk/notice/'+str(i))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PhXZ7FMW8ASL",
        "colab_type": "text"
      },
      "source": [
        "Now to run the function on each URL:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bAzHB-kY8C2z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "startnum = 3581867\n",
        "endnum = startnum-5000\n",
        "#When going backwards you have to add a negative 'step' argument\n",
        "for i in range(startnum, endnum, -1):\n",
        "  #Some URLs are broken so to stop those breaking the scraper we use try/except\n",
        "  try:\n",
        "    #We have to convert to string to add to URL\n",
        "    scraperesult = scrapedetail('https://m.thegazette.co.uk/notice/'+str(i))\n",
        "    print(scraperesult)\n",
        "  except:\n",
        "    print(\"problem\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JkA1optLdI4R",
        "colab_type": "text"
      },
      "source": [
        "## Store the results\n",
        "\n",
        "Earlier we imported pandas - now we use it to store the results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2UJgttIt4ro9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#install library to export files\n",
        "from google.colab import files"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PMsug-CgdGZy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Create a dataframe to store data\n",
        "df = pd.DataFrame(columns=[\"notices\"])\n",
        "\n",
        "startnum = 3391187-50000\n",
        "endnum = startnum-10000\n",
        "#When going backwards you have to add a negative 'step' argument\n",
        "for i in range(startnum, endnum, -1):\n",
        "  #Some URLs are broken so to stop those breaking the scraper we use try/except\n",
        "  try:\n",
        "    #We have to convert to string to add to URL\n",
        "    scraperesult = scrapedetail('https://m.thegazette.co.uk/notice/'+str(i))\n",
        "    #print(scraperesult)\n",
        "    df = df.append(scraperesult, ignore_index=True)\n",
        "  except:\n",
        "    print(\"problem\")\n",
        "\n",
        "#Print the dataframe to check\n",
        "#print(df)\n",
        "#Export to csv\n",
        "df.to_csv(\"notices.csv\")\n",
        "#Download it automatically in case the runtime disconnects before we check and do it manually\n",
        "files.download('notices.csv') "
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}